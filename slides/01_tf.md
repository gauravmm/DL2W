# DL2.0 Workshop

## Introduction to TensorFlow
### Gaurav Manek

## TensorFlow?
  
It is                        It is NOT
●   graph-driven computation    ●   a programming language
    library                     ●   just a neural network
●   support for arrays of           library
    arbitrary dimensions            –   it can do a lot more!
●   filled with common (and     ●   Caffe/MatConvNet
    uncommon!) neural
    network primitives          ●   a substitute for
                                    NumPy/SciPy
    –   convolutions
    –   optimizers (automatic
        differentiation!)
      “graph-driven computation”?
1. Design your model as
   a graph, including the
   training and
   evaluation.
2. Write it in Python
3. TensorFlow will build
   the model on the
   CPU/GPU and run it
   there.

## Some Caveats

● Once the model is initialized (i.e. memory is
  allocated), the graph is immutable.
● Moving data between native Python/C++ and

  TensorFlow is inefficient.
  ● Perform all the computation you can using

    TensorFlow primitives, including loading data
    from disk.
● Adding new TensorFlow primitives is difficult.

## Tensor and Variable

Tensor                           Variable
●   The output of any             ●   Stored matrix of arbitrary
    computation.                      shape.
●   A matrix of arbitrary size.   ●   Can be trainable –
                                      Optimizers are allowed to
●   Can be converted to
                                      change.
    Numpy array.

## Op and Placeholder
               Op                          Placeholder
●   Any computation – takes       ●   Reserved space for
    variables and ops as input,       arbitrary input.
    and (at runtime) produces     ●   An actual value must be
    a Tensor as output.
                                      provided during execution.
●   Any op can be used as a       ●   Unfilled placeholders
    sink/output node. All
                                      cause exceptions.
    dependencies are
    automatically computed.       ●   Typically a numpy array is
                                      expected.
●   Ops and variables can be
    grouped into a single Op.

## Let’s write a computational graph! (1/5)

     Images              Prediction

      Labels

## Let’s write a computational graph! (2/5)

                         Softmax Loss Function
     Images

                                                 Optimizer
      Labels

## Let’s write a computational graph! (3/5)

               Initializer




                             Softmax Loss Function
     Images




                                                     Optimizer
      Labels

## Let’s write a computational graph! (4/5)

               Initializer




                                                     Logging
                             Softmax Loss Function
     Images




                                                     Optimizer
      Labels

## Let’s write a computational graph!             (5/5)        (Sink)
                                                             Target nodes


                       Initializer




                                                                Logging
 (Source)
Placeholder
   Nodes




                                     Softmax Loss Function
              Images




                                                                Optimizer
              Labels
                     cnn/train.py
●   Download the example with:
    git clone https://github.com/gauravmm/DL2W.git
●   Go to the directory and run the example with:
                  python3 workshop.py cnn train
●   You can download pretrained weights using:
              python3 workshop.py --pretrained cnn
●   The first time you run any example, it will download the
    dataset.
●   You need a wired connection or VPN to access the datasets.
   Tensorboard Output
tensorboard --logdir cnn/train_logs
                           Data Precision
●   Supports:
    – tf.int(8|16|32|64)
    – tf.float(16|32|64)
    –   ...
●   tf.float16 offers 10-bit precision, and is a good
    compromise.
●   Caveat:
    –   Half-precision on GPU requires hardware support.
         ●    TITAN X / Tesla cards offer this.
               Data Precision +
            Automatic Differentiation
●   Almost all operations support automatic
    differentiation.
    –   Optimizers use this automatically.
●   Caveats: Precision Issues!
    –   Vanishing Gradients – Avoid tf.softmax(tf.softmax(x))
         ● The cross-entropy loss functions have this by default!


    –   Order of Operations
         ● tf.reduce_sum(tf.log(x)) is better than

           tf.log(tf.reduce_prod(x))
                     Supervisor
●   Automates loading, initialization, summary
    writing.
●   Refer to cnn.py:46-51 for example.
●   If any variables are saved, they are
    transparently loaded when the managed
    session is created.
    –   with sv.managed_session() as sess:
           Logging Training Progress
●   Insert summary ops in the graph.
     – tf.summary.*
●   Run the summary op
    –   You can run it with some computation (e.g. training) or by
        itself.
    –   Merge all summaries using tf.summary.merge(_all)?
    –   Output of this is a tf.Summaries object.
●   Save the summary object
    –   Use a tf.summary.FileWriter object.
           Logging Training Progress
●   Insert summary ops in the graph.
     – tf.summary.*
●   Run the summary op


              Use a
    –   You can run it with some computation (e.g. training) or by
        itself.
    –   Merge all summaries using tf.summary.merge(_all)?


            Supervisor!
    –   Output of this is a tf.Summaries object.
●   Save the summary object
    –   Use a tf.summary.FileWriter object.
               What Ops can I Use?
●   tf.nn
    –   Ops that perform computation.
    –   An interface to the underlying implementation.
●   tf.layers
    –   Neural network layers!
●   tf.contrib
    –   A huge variety of ops that handle distributions, audio,
        kernel methods, linear algebra, linear optimization,
        sparse matrices, sequence-to-sequence, etc.
                    Some Op Caveats
●   Batch Normalization
    –   Additional UPDATE_OPS are created, and must be run
        with the optimizer.
●   <tensor>.get_shape() vs tf.shape(<tensor>)
    –   <tensor>.get_shape()
        ●   Shape at construction time
        ●   Unknown dimensions are ?
    –   tf.shape(<tensor>)
        ●   Is an op.
        ●   Shape at runtime, when all dimensions are known.
                        In Summary
●   TensorFlow’s computation model
●   Comptuational Graph cannot be changed after it is
    initialized
●   Automatic dependency calculations
    –   You tell it what sinks you want,
    –   If you’re missing any sources, it barfs
●   Training supervisors automate a lot of the overhead.
●   Some Ops are special – read the documentation!

